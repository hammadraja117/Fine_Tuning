{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "606986be-fd43-4b0f-b69b-02250e57e4b0",
      "metadata": {
        "id": "606986be-fd43-4b0f-b69b-02250e57e4b0"
      },
      "source": [
        "### Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae9d6495-af97-405d-b4d6-63b322cb82d5",
        "outputId": "0f1b88ef-c08c-4efa-c91e-cd7e4bdcd715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.3/230.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4",
      "metadata": {
        "id": "bacdda41-708b-4af8-88a9-5056cbd08bf4"
      },
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "505ca9a3-8c27-442e-bca6-154a65186d01",
      "metadata": {
        "tags": [],
        "id": "505ca9a3-8c27-442e-bca6-154a65186d01"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.schema import format_document\n",
        "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from operator import itemgetter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.cache import RedisSemanticCache\n",
        "from langchain.cache import CassandraSemanticCache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3",
      "metadata": {
        "id": "180f8d4e-a8bf-4389-b046-9827b310a3b3"
      },
      "source": [
        "### Load quantized Llama 2 7B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='NousResearch/Llama-2-7b-chat-hf'"
      ],
      "metadata": {
        "id": "oMB28tQbkqD1"
      },
      "id": "oMB28tQbkqD1",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up semantic cache with Redis\n",
        "# semantic_cache = RedisSemanticCache(redis_url=\"redis://localhost:6379\", embedding=HuggingFaceEmbeddings(model_name=model_name))\n"
      ],
      "metadata": {
        "id": "osO_9w5ZjwGy"
      },
      "id": "osO_9w5ZjwGy",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "80f94e97-7e58-4253-9376-73af6f36e139",
      "metadata": {
        "tags": [],
        "id": "80f94e97-7e58-4253-9376-73af6f36e139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "ea0eb16798d7493f98b24e00bb32f721",
            "53acfcb4569f4da0802f0a1da5bd3175",
            "b78ae784e1ac4fa39fa251b3c0e10dcb",
            "54d2de0cd5644cb994fff779c78e090f",
            "dfd5ecf258fb4c56961e94a063fa5749",
            "a01115434a7e4299a7dff7fd1e335219",
            "5a8c1c54a8684c9991a374cd3124636f",
            "45c11b750e4c4608b976d71bc2729aed",
            "b73437e7b3244aeaa5c133d41e131a16",
            "1c9a1d39943f4cc79bbcd077b279fc96",
            "1b67d8bd749e44578a48ccad9eafa275"
          ]
        },
        "outputId": "1d0bcad9-d746-4c37-8fe0-e989048ffe6b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea0eb16798d7493f98b24e00bb32f721"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69",
      "metadata": {
        "id": "e7fb199a-a537-4bd7-9888-d43a84c8ff69"
      },
      "source": [
        "### Count number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "91d2a86e-69e8-496f-b388-853168537c20",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91d2a86e-69e8-496f-b388-853168537c20",
        "outputId": "186adf2c-08a0-45d4-e1b0-c7ea7aa6a02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable model parameters: 262410240\n",
            "all model parameters: 3500412928\n",
            "percentage of trainable model parameters: 7.50%\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5",
      "metadata": {
        "id": "5a38c760-f5c8-49c6-9c0c-80719557fee5"
      },
      "source": [
        "### Build Mistral text generation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b",
      "metadata": {
        "tags": [],
        "id": "8c613429-9e6c-4a1e-bc9c-579eb152434b"
      },
      "outputs": [],
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.00,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7",
      "metadata": {
        "tags": [],
        "id": "c859dd05-9114-42f1-81f2-52a28b7efdd7"
      },
      "outputs": [],
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6",
      "metadata": {
        "id": "e3a07789-78f5-498c-987b-9ed3eb459fe6"
      },
      "source": [
        "### Load and chunk documents. Load chunked documents into FAISS index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "35e625a4-8d25-453e-bef0-435a6e1aa135",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "35e625a4-8d25-453e-bef0-435a6e1aa135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b7ab26-4a6c-4a20-ca0e-0e1a7b0f4cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-freefont-ttf is already the newest version (20120503-10build1).\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libcairo-gobject2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libdbus-glib-1-2 is already the newest version (0.112-2build1).\n",
            "libegl1 is already the newest version (1.4.0-1).\n",
            "libenchant-2-2 is already the newest version (2.3.2-1ubuntu2).\n",
            "libepoxy0 is already the newest version (1.5.10-1).\n",
            "libevdev2 is already the newest version (1.12.1+dfsg-1).\n",
            "libevent-2.1-7 is already the newest version (2.1.12-stable-1build3).\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libgles2 is already the newest version (1.4.0-1).\n",
            "libglx0 is already the newest version (1.4.0-1).\n",
            "libgudev-1.0-0 is already the newest version (1:237-2build1).\n",
            "libhyphen0 is already the newest version (2.8.8-7build2).\n",
            "libicu70 is already the newest version (70.1-2).\n",
            "libjpeg-turbo8 is already the newest version (2.1.2-0ubuntu1).\n",
            "liblcms2-2 is already the newest version (2.12~rc1-2build2).\n",
            "libmanette-0.2-0 is already the newest version (0.2.6-3build1).\n",
            "libnspr4 is already the newest version (2:4.32-3build1).\n",
            "libopengl0 is already the newest version (1.4.0-1).\n",
            "libopenjp2-7 is already the newest version (2.4.0-6).\n",
            "libopus0 is already the newest version (1.3.1-0.1build2).\n",
            "libpng16-16 is already the newest version (1.6.37-3build5).\n",
            "libproxy1v5 is already the newest version (0.4.17-2).\n",
            "libsecret-1-0 is already the newest version (0.20.5-2).\n",
            "libwoff1 is already the newest version (1.0.2-1build4).\n",
            "libxcb-shm0 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcursor1 is already the newest version (1:1.2.0-2build4).\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrender1 is already the newest version (1:0.9.10-1build4).\n",
            "libxtst6 is already the newest version (2:1.2.3-1build4).\n",
            "xfonts-scalable is already the newest version (1:1.0.3-1.2ubuntu1).\n",
            "fonts-ipafont-gothic is already the newest version (00303-21ubuntu1).\n",
            "fonts-tlwg-loma-otf is already the newest version (1:0.7.3-1).\n",
            "fonts-unifont is already the newest version (1:14.0.01-1).\n",
            "fonts-wqy-zenhei is already the newest version (0.9.45-8).\n",
            "libffi7 is already the newest version (3.3-5ubuntu1).\n",
            "libx264-163 is already the newest version (2:0.163.3060+git5db6aa6-2build1).\n",
            "xfonts-cyrillic is already the newest version (1:1.0.5).\n",
            "fonts-noto-color-emoji is already the newest version (2.042-0ubuntu0.22.04.1).\n",
            "gstreamer1.0-plugins-base is already the newest version (1.20.1-1ubuntu0.1).\n",
            "gstreamer1.0-plugins-good is already the newest version (1.20.3-0ubuntu1.1).\n",
            "libatomic1 is already the newest version (12.3.0-1ubuntu1~22.04).\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.7).\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
            "libgbm1 is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "libgdk-pixbuf-2.0-0 is already the newest version (2.42.8+dfsg-1ubuntu0.2).\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.2).\n",
            "libgstreamer-gl1.0-0 is already the newest version (1.20.1-1ubuntu0.1).\n",
            "libgstreamer-plugins-base1.0-0 is already the newest version (1.20.1-1ubuntu0.1).\n",
            "libgstreamer1.0-0 is already the newest version (1.20.3-0ubuntu1).\n",
            "libgtk-3-0 is already the newest version (3.24.33-1ubuntu2).\n",
            "libharfbuzz-icu0 is already the newest version (2.7.4-1ubuntu3.1).\n",
            "libharfbuzz0b is already the newest version (2.7.4-1ubuntu3.1).\n",
            "libnotify4 is already the newest version (0.7.9-3ubuntu5.22.04.1).\n",
            "libnss3 is already the newest version (2:3.68.2-0ubuntu1.2).\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpangocairo-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-egl1 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-server0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwebpdemux2 is already the newest version (1.2.2-2ubuntu0.22.04.2).\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libxml2 is already the newest version (2.9.13+dfsg-1ubuntu0.3).\n",
            "libxslt1.1 is already the newest version (1.1.34-4ubuntu0.22.04.1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "gstreamer1.0-libav is already the newest version (1.20.3-0ubuntu1).\n",
            "gstreamer1.0-plugins-bad is already the newest version (1.20.3-0ubuntu1.1).\n",
            "libsoup-3.0-0 is already the newest version (3.0.7-0ubuntu1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!playwright install\n",
        "!playwright install-deps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Articles to index\n",
        "# articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
        "#             \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
        "#             \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
        "#             \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
        "#             \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
        "\n",
        "\n",
        "articles = [\"https://stackoverflow.com/questions/74893430/scrapping-a-forum-page-to-get-all-responses-and-user-info\",\n",
        "            \"https://stackoverflow.com/questions/66071647/python-scrape-forum-for-title-for-each-post\",\n",
        "            \"https://stackoverflow.com/questions/48499652/unable-to-collect-titles-from-a-webpage-in-the-right-way?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/72850215/scrape-the-title-from-ecommerce-site-using-selenium-python?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/48661203/find-page-title-in-selenium-python?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/59838917/how-to-find-title-xyz-element-with-selenium-python?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/32751250/selenium-python-2-7-how-to-find-element-that-only-has-a-title?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/63237685/how-to-select-an-element-with-no-title-using-selenium-and-python?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/52800322/select-web-element-only-with-title-tag-in-selenium-python?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/48011212/selenium-find-element-by-title-that-contains?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/72484590/gpt-3-keywords-extractor?rq=3\",\n",
        "            \"https://stackoverflow.com/questions/71618602/finetuning-gpt-3-on-windows?rq=3\"]\n",
        "# Scrapes the blogs above\n",
        "loader = AsyncChromiumLoader(articles)\n",
        "docs = loader.load()\n",
        "\n",
        "type(docs[0])\n",
        "\n",
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000,\n",
        "                                      chunk_overlap=200)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "# retriever = db.as_retriever(search_kwargs={'k': 3})\n",
        "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.2, 'k':3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvbWQAb-MSR9",
        "outputId": "5a928d0a-ca50-43f1-8b1f-0d8e86b9b93c"
      },
      "id": "bvbWQAb-MSR9",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1720, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 12141, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1669, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2927, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1813, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1494, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 5024, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1733, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1624, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1727, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1569, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1678, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1537, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 7109, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1533, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1181, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1483, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1654, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a",
      "metadata": {
        "tags": [],
        "id": "79a2e41f-aee3-47ff-92a1-74970f3b313a"
      },
      "outputs": [],
      "source": [
        "# import nest_asyncio\n",
        "# import requests\n",
        "\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# # Load data from the GitHub repository file\n",
        "# github_raw_url = \"https://raw.githubusercontent.com/hammadraja117/Solutyics/main/Solutions.txt\"\n",
        "# response = requests.get(github_raw_url)\n",
        "# data = response.text\n",
        "\n",
        "# # Articles to index\n",
        "# articles = [data]\n",
        "\n",
        "# # # Scrapes the blogs above\n",
        "# loader = AsyncChromiumLoader(articles)\n",
        "# docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# docs"
      ],
      "metadata": {
        "id": "oih3yE7LRkw1"
      },
      "id": "oih3yE7LRkw1",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "ff328fea-b7c7-4ca3-915c-39c0ebaa2f7a"
      },
      "outputs": [],
      "source": [
        "# Converts HTML to plain text\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000,\n",
        "                                      chunk_overlap=200)\n",
        "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "db = FAISS.from_documents(chunked_documents,\n",
        "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "retriever = db.as_retriever(search_kwargs={'k': 3})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install cassandra-driver"
      ],
      "metadata": {
        "id": "np3SIuqcc6Jp"
      },
      "id": "np3SIuqcc6Jp",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/pcmanus/ccm.git\n"
      ],
      "metadata": {
        "id": "3LgQOo3Hd4tP"
      },
      "id": "3LgQOo3Hd4tP",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !service cassandra status"
      ],
      "metadata": {
        "id": "vptyfwXbf6RB"
      },
      "id": "vptyfwXbf6RB",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !service cassandra start"
      ],
      "metadata": {
        "id": "j0xY7zJrfv3L"
      },
      "id": "j0xY7zJrfv3L",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from cassandra.cluster import Cluster\n",
        "\n",
        "# cluster = Cluster(['127.0.0.1'])\n",
        "# session = cluster.connect()\n",
        "\n",
        "# # Continue with your existing code\n",
        "# cassandra_cache = CassandraSemanticCache(\n",
        "#     session=session,\n",
        "#     keyspace=keyspace,\n",
        "#     embedding=HuggingFaceEmbeddings(model_name=model_name),\n",
        "#     table_name=\"cass_sem_cache\",\n",
        "# )\n",
        "\n",
        "# set_llm_cache(cassandra_cache)"
      ],
      "metadata": {
        "id": "E3P2GQUyZ2H0"
      },
      "id": "E3P2GQUyZ2H0",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFHGRhELsTOU",
        "outputId": "24e09704-29fe-4591-8037-1d657ee2ab25"
      },
      "id": "dFHGRhELsTOU",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7a0dba3725f0>, search_kwargs={'k': 3})"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG with conversation"
      ],
      "metadata": {
        "id": "BO471y2Bl6OL"
      },
      "id": "BO471y2Bl6OL"
    },
    {
      "cell_type": "code",
      "source": [
        "_template = \"\"\"[INST] Intruction: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question: [/INST]\"\"\"\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ],
      "metadata": {
        "id": "1MCLd2qnmQ3k"
      },
      "id": "1MCLd2qnmQ3k",
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"[INST]You are ChatBot, specifically trained on the problems and solutions documented within the articles loaded exclusively from: {context}. Your primary function is to provide direct and concise answers based solely on this data.\n",
        "Data Source:\n",
        "{context}\n",
        "Retrieval:\n",
        "Focus on retrieving only the relevant documents that directly address the user's query.\n",
        "Answer Generation:\n",
        "Generate answers strictly based on the retrieved documents, without utilizing any external knowledge or pre-trained information. If a question has multiple answers, generate all the answers provided in the context.\n",
        "Limitations:\n",
        "If the user's question cannot be answered by the loaded data, don't hallucinate it, respond with: \"I'm sorry, I couldn't find relevant information about that.\"\n",
        "Accuracy:\n",
        "Avoid making assumptions or generating information not present in the loaded data.\n",
        "Focus:\n",
        "Stay on topic, addressing only the provided problems and solutions within the loaded articles.\n",
        "Origin Information:\n",
        "If asked who created you, respond: \"I was created by AI engineers at Solutyics to assist with specific queries related to the company's data.\"\n",
        "start Conversation:\n",
        "when someone greets you like, \"hello\", \"hi\". then Greets them back like \" hello sir, How may i assist you today?\"\n",
        "\n",
        "Remember:\n",
        "Provide direct and concise answers, avoiding unnecessary introductions or generic statements.\n",
        "Strictly adhere to the loaded data and avoid any external knowledge or assumptions.\n",
        "Offer a clear response when information is unavailable.\n",
        "{context}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "4Y9Ohprjm1EV"
      },
      "id": "4Y9Ohprjm1EV",
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    # Assuming `format_document` uses appropriate formatting techniques\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)"
      ],
      "metadata": {
        "id": "0WVM6O4pm_23"
      },
      "id": "0WVM6O4pm_23",
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Memory"
      ],
      "metadata": {
        "id": "MQxQBiRQuqfh"
      },
      "id": "MQxQBiRQuqfh"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")"
      ],
      "metadata": {
        "id": "sLycesYFuwVq"
      },
      "id": "sLycesYFuwVq",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "    \"standalone_question\": {\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
        "    }\n",
        "    | CONDENSE_QUESTION_PROMPT\n",
        "    | mistral_llm\n",
        "    | StrOutputParser(),\n",
        "}\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "    # Remove hardcoded URL from template, provide as input:\n",
        "    \"https\": lambda x: github_raw_url  # Assuming `https` holds the URL\n",
        "}\n",
        "\n",
        "# ... loading and retrieval steps ...\n",
        "\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "    # Provide URL as input here:\n",
        "    \"https\": lambda x: x[\"https\"]\n",
        "}\n",
        "\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | mistral_llm,\n",
        "    \"docs\": itemgetter(\"docs\"),\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ],
      "metadata": {
        "id": "r4WDcf5ruyNa"
      },
      "id": "r4WDcf5ruyNa",
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"How to finetune gpt-3 on windows\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "niZA5Mq4u5rP",
        "outputId": "1a10c4d1-0160-4b10-ac63-4e7685a1c1df"
      },
      "id": "niZA5Mq4u5rP",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThank you for providing more context to your question. Based on the information provided, it appears that you are trying to fine-tune GPT-3 on a Windows machine using the OpenAI API. However, the proposed CLI commands do not work in the Windows CMD interface, and you are unable to find any documentation on how to fine-tune GPT3 using a \"regular\" Python script.\\n\\nUnfortunately, it seems that fine-tuning GPT-3 on a Windows machine is not possible using the current documentation and tools provided by OpenAI. The documentation only provides instructions for fine-tuning on Linux and macOS systems, and it does not provide any information on how to do so on Windows.\\n\\nI apologize for any confusion or inconvenience this may cause. However, I hope this answer helps clarify things for you. If you have any further questions or concerns, please feel free to ask!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"error 201\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "vBFywGUHSOIP",
        "outputId": "cd0500a0-a6e5-4eda-f032-9ad1861906da"
      },
      "id": "vBFywGUHSOIP",
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nResponse:\\nHello! I'm here to help you with your question. Based on the data provided, error 201 is a status code indicating that the request was successfully fulfilled and resulted in one or possibly multiple new resources being created. It's likely that the creation of these new resources was successful and completed as expected. However, without additional context or information, I cannot provide a definitive answer to your question. Can you please provide more details or clarify your question?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"ValueError: Traceback (most recent call last) <ipython-input-15-d27c936e2293> in <module>()\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "rDzUK1BSBNkB",
        "outputId": "f7fea61c-319c-4b1d-8bf0-57742948c0f5"
      },
      "id": "rDzUK1BSBNkB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBased on the loaded articles, the answer to your question is:\\n\\nThe \"ValueError: Traceback (most recent call last)\" error message indicates that there is an issue with the way the code is written, likely in the most recent line of code being executed. This error message is typically seen when there is a syntax or indentation issue in the code, causing the interpreter to throw a \"Traceback\" error. In this case, it\\'s possible that there is a mismatch between the expected indentation or syntax of the code and what is actually written in the code. To resolve this issue, the developer can review the code carefully and make any necessary changes to ensure that it is properly formatted and indented.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"How to make biryani?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "r-KnLBFUCldB",
        "outputId": "141ac5b2-2cbf-4ae7-9486-26b61a9566a2"
      },
      "id": "r-KnLBFUCldB",
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm here to help you with your question. However, I must inform you that I'm just an AI assistant and not a professional chef, so my answer may not be the most detailed or accurate. That being said, here's a basic recipe for making biryani:\\n\\nIngredients:\\n\\n* 2 cups basmati rice\\n* 2 tablespoons ghee or oil\\n* 1 teaspoon cumin seeds\\n* 1 teaspoon coriander seeds\\n* 1 teaspoon bay leaves\\n* 1 teaspoon cardamom powder\\n* 1 teaspoon cinnamon powder\\n* 1/2 teaspoon turmeric powder\\n* Salt, to taste\\n* 2 cups water\\n* 1 cup meat or vegetables of your choice (optional)\\n\\nInstructions:\\n\\n1. Heat the ghee or oil in a large saucepan over medium heat. Once hot, add the cumin seeds, coriander seeds, bay leaves, cardamom powder, and cinnamon powder. Let them sizzle for a few seconds until fragrant.\\n2. Add the turmeric powder and salt to the saucepan and mix well.\\n3. Add the rice to the saucepan and stir to coat the rice with the spice mixture.\\n4. Add the water to the saucepan and bring to a boil.\\n5. Reduce the heat to low, cover the saucepan with a tight-fitting lid, and let it simmer for 15-20 minutes or until the rice is cooked and fluffy.\\n6. If using meat or vegetables, add them to the saucepan after the rice has cooked for 5-7 minutes.\\n7. Let the biryani rest for 5-10 minutes before serving.\\n\\nNote: You can customize the spices and seasonings according to your preference. Also, you can use chicken, lamb, or vegetables instead of meat for a vegetarian biryani.\\n\\nI hope this helps you get started with making biryani!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "aS8F407JvKdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43e9b5c-d915-4844-d3f5-2ca97852d383"
      },
      "id": "aS8F407JvKdE",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='error 201'),\n",
              "  AIMessage(content=\"\\nResponse:\\nHello! I'm here to help you with your question. Based on the data provided, error 201 is a status code indicating that the request was successfully fulfilled and resulted in one or possibly multiple new resources being created. It's likely that the creation of these new resources was successful and completed as expected. However, without additional context or information, I cannot provide a definitive answer to your question. Can you please provide more details or clarify your question?\")]}"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"where is solutyics located?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "ZUpmbQ1niDVM",
        "outputId": "862f021d-0843-4ffa-f5bd-d66baab8ad95"
      },
      "id": "ZUpmbQ1niDVM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! Welcome to Solutyics! 👋\\n\\nI see that you're interested in learning more about our company and the services we offer. As a helpful assistant, I'm here to provide you with the information you need.\\n\\nFirstly, let me tell you that Solutyics is a technology company that specializes in providing automated chat solutions, including chatbots, IVRS, and RAGer. We help businesses streamline their operations and improve customer satisfaction by leveraging the power of artificial intelligence and machine learning.\\n\\nOur chatbots offer a personalized and efficient way to engage with customers, providing timely and relevant responses to their inquiries. IVRS, on the other hand, allows businesses to automate routine tasks and processes, freeing up valuable time and resources for more important things. And RAGer, our flagship product, enables users to access and interact with their company's documents in a user-friendly and efficient manner.\\n\\nWe understand that data privacy and security are crucial for any business, which is why we prioritize these aspects in all of our solutions. At Solutyics, we believe in unlocking growth through analytics and AI synergy, and we're committed to helping businesses achieve their goals.\\n\\nIf you have any further questions or would like to learn more about our products and services, please feel free to ask! 😊\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vABN13K198iB",
        "outputId": "18a016f6-2966-4058-e098-cd8d0746d3a8"
      },
      "id": "vABN13K198iB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': \"I don't know the solution to this problem.\",\n",
              " 'docs': [Document(page_content='25\\n\\nFind and click an element by title using Python and Selenium\\n\\n2\\n\\nSelenium/python 2.7 How to find element that only has a title?\\n\\n5\\n\\nSelenium - Finding element that has title\\n\\n1\\n\\nSelenium Python XPATH I am trying to find the input tag which has some text in\\nthe title attribute\\n\\n1\\n\\nFind title of element\\n\\n2\\n\\nFind page title in selenium-python\\n\\n2\\n\\nHow to find title=\"xyz\" element with Selenium (Python)\\n\\n0\\n\\nHow to find a web element in selenium that contains a text using python\\n\\n####  Hot Network Questions', metadata={'source': 'https://stackoverflow.com/questions/48011212/selenium-find-element-by-title-that-contains?rq=3'}),\n",
              "  Document(page_content='2\\n\\nGet all the attributes \"title\" with selenium in python\\n\\n0\\n\\nGet title from <a> from an interation Selenium Python\\n\\n1\\n\\nHow to select an element with no title using Selenium and Python?\\n\\n0\\n\\nPython can\\'t capture element by title selenium\\n\\n0\\n\\nUsing Selenium to get the information within title= \\' \\'\\n\\n1\\n\\nHow to locate title in html head with the title name XPATH\\n\\n####  Hot Network Questions', metadata={'source': 'https://stackoverflow.com/questions/59838917/how-to-find-title-xyz-element-with-selenium-python?rq=3'}),\n",
              "  Document(page_content=\"0\\n\\nSelect element with selenium python\\n\\n3\\n\\nselenium find element by title that contains?\\n\\n1\\n\\nHow to select an element with no title using Selenium and Python?\\n\\n0\\n\\nPython can't capture element by title selenium\\n\\n0\\n\\nHow to select the text in the div using Selenium?\\n\\n####  Hot Network Questions\", metadata={'source': 'https://stackoverflow.com/questions/52800322/select-web-element-only-with-title-tag-in-selenium-python?rq=3'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "To search for an element by title using Selenium in Python, you can use the `find_element_by_title()` method of the WebDriver object. This method takes the title of the element as an argument and returns the WebElement object corresponding to the element. Here's an example code snippet:\n",
        "```python\n",
        "from selenium import webdriver\n",
        "\n",
        "# create a new instance of the Chrome driver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# navigate to a webpage\n",
        "driver.get(\"https://www.example.com\")\n",
        "\n",
        "# find the element with title \"My Title\"\n",
        "my_element = driver.find_element_by_title(\"My Title\")\n",
        "\n",
        "# print the text content of the element\n",
        "print(my_element.text)\n",
        "```\n",
        "If you want to find all elements with a specific title, you can use the `find_elements_by_title()` method instead. This method returns a list of WebElement objects corresponding to the elements with the specified title.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v6KfIkDtJYin"
      },
      "id": "v6KfIkDtJYin",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-gpu.m114",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-gpu:m114"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea0eb16798d7493f98b24e00bb32f721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53acfcb4569f4da0802f0a1da5bd3175",
              "IPY_MODEL_b78ae784e1ac4fa39fa251b3c0e10dcb",
              "IPY_MODEL_54d2de0cd5644cb994fff779c78e090f"
            ],
            "layout": "IPY_MODEL_dfd5ecf258fb4c56961e94a063fa5749"
          }
        },
        "53acfcb4569f4da0802f0a1da5bd3175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a01115434a7e4299a7dff7fd1e335219",
            "placeholder": "​",
            "style": "IPY_MODEL_5a8c1c54a8684c9991a374cd3124636f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b78ae784e1ac4fa39fa251b3c0e10dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c11b750e4c4608b976d71bc2729aed",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b73437e7b3244aeaa5c133d41e131a16",
            "value": 2
          }
        },
        "54d2de0cd5644cb994fff779c78e090f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c9a1d39943f4cc79bbcd077b279fc96",
            "placeholder": "​",
            "style": "IPY_MODEL_1b67d8bd749e44578a48ccad9eafa275",
            "value": " 2/2 [01:02&lt;00:00, 28.77s/it]"
          }
        },
        "dfd5ecf258fb4c56961e94a063fa5749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01115434a7e4299a7dff7fd1e335219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8c1c54a8684c9991a374cd3124636f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45c11b750e4c4608b976d71bc2729aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b73437e7b3244aeaa5c133d41e131a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c9a1d39943f4cc79bbcd077b279fc96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b67d8bd749e44578a48ccad9eafa275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}